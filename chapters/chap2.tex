%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Chap 2. Background
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Background}\label{chapter2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reinforcement Learning} \label{chap2:sec1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Reinforcement Learning (RL) is a framework in which an agent interacts with an environment and learns a policy to maximize cumulative rewards.
The agent observes the state of the environment, takes actions, and receives rewards based on those actions.
This process is formalized as a Markov Decision Process (MDP) \cite{MDP}, which provides a formal strcuture for modeling decision-making problems.
An MDP is defined by a tuple \((\mathcal{S}, \mathcal{A}, P, R, \gamma)\), where $\mathcal{S}$ is the set of states, $\mathcal{A}$ is the set of actions, $P$ is the state transition probability function, $R$ is the reward function, and $\gamma \in [0, 1)$ is the discount factor.
The objective of RL is to find an optimal policy $\pi^*$ that maximizes the expected cumulative reward, defined as:
\begin{equation}
  \begin{aligned}
    \theta^* &= \arg\max J(\theta) \\
    J(\theta) &= \mathbb{E}_{\tau \sim \pi_\theta} \left[\sum^T_{t = 0} r_t \right]
  \end{aligned}
\end{equation}
The policy $\pi_\theta$ is assumed to be a differentiable function parmeterized by $\theta$, denoted as $\pi_\theta(a|s)$, which represents the probability of taking ation $a$ given state $s$.
The expectation $\mathbb{E}_{\tau \sim \pi_\theta}$ is taken over the trajectories $\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots, s_T)$ generated by following the policy $\pi_\theta$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Policy Gradient Methods} \label{chap2:sec2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Since the policy is differentiable, its gradient can be expressed using the likelihood ratio trick:
\begin{equation}
  \begin{aligned}
    \nabla_\theta \pi_\theta(s, a)
    &= \pi_\theta(s, a) \frac{\nabla_\theta \pi_\theta(s, a)}{\pi_\theta(s, a)} \\
    &= \pi_\theta(s, a) \nabla_\theta \log \pi_\theta(s, a)
  \end{aligned}
\end{equation}
This term $\nabla_\theta \log \pi_\theta(s, a)$ is referred to as the score function.
Although we previously defined the objective function $J(\pi_\theta)$ as the expected cumulative reward, we now consider a simplified case to facilitate explanation.
A one-step MDP, in which the agent takes an action from the initial state, receives a reward, and the episode terminates immediately.
Then, the objective function can be written as ($d$ is the initial state distribution):
\begin{equation}
  \begin{aligned}
    J(\theta)
    &= \mathbb{E}_{\pi_\theta}[r] \\
    &= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi_\theta(s, a) R_{s ,a}
  \end{aligned}
\end{equation}
The gradient of the objective function can be computed as follows:
\begin{equation}
  \begin{aligned}
    \nabla_\theta J(\theta)
    &= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \nabla_\theta \pi_\theta(s, a) R_{s, a} \\
    &= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi_\theta(s, a) \nabla_\theta \log \pi_\theta(s, a) R_{s, a} \\
    &= \mathbb{E}_{\pi_\theta} [\nabla_\theta \log \pi_\theta(s, a) r]
  \end{aligned}
\end{equation}
The policy gradient theorem generalizes this result to multi-step MDP, where the objective function is defined as the expected cumulative reward over multiple time steps.
In other words, it replaces the instantaneous reward $r$ with the long-term value $Q^{\pi_\theta} (s, a)$, the action value function.
\begin{theorem}[Policy Gradient Theorem] \label{chap2:thm:pg}
  \begin{equation}
    \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} [\nabla_\theta \log \pi_\theta(s, a) Q^{\pi_\theta}(s, a)]
  \end{equation}
\end{theorem}

\subsection{REINFORCE}

In practice, the exact action value function $Q^{\pi_\theta} (s, a)$ is typically unknown.
Accordingly, the estimated return $G_t$ can be used as an approximation of the action value function.
In other words, the action value function $Q^{\pi_\theta} (s, a)$ can be replaced with the return $G_t$ from real sample trajectories using the Monte Carlo method.
\begin{equation}
  \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) G_t]
\end{equation}
This leads to the Monte-Carlo policy gradient method, commonly known as REINFORCE.
However, REINFORCE suffers from high variance in the gradient estimates, due to its reliance on a full trajectory.

\subsection{Actor-Critic}

A common approach to reducing the variance is to use a critic approximates the action value function, $Q_w(s, a) \approx Q^{\pi_\theta}(s, a)$.
\begin{equation}
  \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} [\nabla_\theta \log \pi_\theta(s, a) Q_w(s, a)]
\end{equation}
This is the basic idea of the Actor-Critic methods.
Actor updates the policy parameters $\theta$ and a critic updates the value function parameters $w$.

\subsection{Advantage Actor-Critic}
To further reduce the variance, we can introduce a baseline function $B(s)$.
Importantly, subtracting a baseline from the action value function does not change the gradient because its gradient is zero.
\begin{equation}
  \begin{aligned}
    \mathbb{E}_{\pi_\theta} [\nabla_\theta \log \pi_\theta(s, a) B(s)]
    &= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi_\theta(s, a) \nabla_\theta \log \pi_\theta(s, a) B(s) \\
    &= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \nabla_\theta \pi_\theta(s, a) B(s) \\
    &= \sum_{s \in \mathcal{S}} d(s) B(s) \nabla_\theta \sum_{a \in \mathcal{A}} \pi_\theta(s, a) \\
    &= 0
  \end{aligned}
\end{equation}
Therefore, subtracting a baseline function from the action value function not only leaves the gradient unchanged but also reduces its variance.
\begin{equation} \label{chap2:eq:pg_adv}
  \begin{aligned}
    A^{\pi_\theta}(s, a) &= Q^{\pi_\theta}(s, a) - V^{\pi_\theta}(s) \\
    \nabla_\theta J(\theta) &= \mathbb{E}_{\pi_\theta} [\nabla_\theta \log \pi_\theta(s, a) A^{\pi_\theta}(s, a)]
  \end{aligned}
\end{equation}
However, since the exact advantage function is generally inaccessible, both the action value function $Q^{\pi_\theta}(s, a)$ and the value function $V^{\pi_\theta}(s)$ must be approximated.
One common approach is to use the temporal difference (TD) error $\delta^{\pi_\theta}$, which is an unbiased estimator of the advantage function.

\subsection{TD Actor-Critic}

The TD error is defined as:
\begin{equation}
  \begin{aligned}
    \delta^{\pi_\theta} &= r+ \gamma V^{\pi_\theta}(s') - V^{\pi_\theta}(s) \\
    \mathbb{E}_{\pi_\theta}[\delta^{\pi_\theta}|s, a] &= Q^{\pi_\theta}(s, a) - V^{\pi_\theta}(s) = A^{\pi_\theta}(s, a)
  \end{aligned}
\end{equation}
Thus, we can use $\delta^{\pi_\theta}$ in place of the advantage function in the policy gradient formula.
\begin{equation}
  \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} [\nabla_\theta \log \pi_\theta(s, a) \delta^{\pi_\theta}]
\end{equation}
Hence, the TD error provides a practical and efficient way to estimate the advantage function using only the value function.
To summarize, the policy gradient has many equivalent formulations:
\begin{equation}
  \begin{aligned}
    \nabla_\theta J(\theta)
    &= \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) v_t] &\quad \text{REINFORCE} \\
    &= \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) Q_W(s, a)] &\quad \text{Q Actor-Critic} \\
    &= \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) A^{\pi_\theta}(s, a)] &\quad \text{Advantage Actor-Critic} \\
    &= \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \delta^{\pi_\theta}] &\quad \text{TD Actor-Critic}
  \end{aligned}
\end{equation}

\subsection{Proximal Policy Optimization (PPO)}

The methods introduced above are all on-policy: training samples are collected using the same policy that we want to optimize.
However, on-policy methods can be inefficient in terms of sample usage and may suffer from instability due to large updates.
Proximal Policy Optimization (PPO) \cite{PPO} addresses these issue by incorporating importance sampling to reuse data collected from the old policy and introducing a clipped surrogate objective function to prevent large policy updates that may cause divergence.
Based on the Advantage Actor-Critic formulation \cref{chap2:eq:pg_adv}, the policy gradient can be interpreted as optimizing the following objective function:
\begin{equation}
  J(\theta) = \mathbb{E}_{\pi_\theta}[\log \pi_\theta(s, a) A^{\pi_\theta}(s, a)]
\end{equation}
This formulation assumes that data is collected from the current policy.  
However, in order to improve sample efficiency by reusing data from a previous policy $\pi_{\theta_\text{old}}$, we can apply importance sampling.
So, the objective function can be rewritten as:
\begin{equation}
  J^{\text{TRPO}}(\theta) = \mathbb{E}_{\pi_{\theta_\text{old}}} \left[ \frac{\pi_\theta (s, a)}{\pi_{\theta_\text{old}}(s, a)}A^{\pi_{\theta_\text{old}}}(s, a) \right]
\end{equation}
This objective was introduced in Trust Region Policy Optimization (TRPO) \cite{TRPO}, the predecessor of PPO.
In this formulation, $r(\theta) = \frac{\pi_\theta (s, a)}{\pi_{\theta_\text{old}}(s, a)}$ represents the probability ratio between old and new policies.
While, this allows us to reuse data from the old policy, maximizing this objective without any constraint can lead to large updates and unstable training.
To prevent such instability, PPO imposes the constraint by forcing $r(\theta)$ to stay within a small interval $[1 - \epsilon, 1 + \epsilon]$.
\begin{equation}
  J^{\text{PPO}}(\theta) = \mathbb{E}_{\pi_{\theta_\text{old}}} \left[ \min \left( r(\theta) A^{\pi_{\theta_\text{old}}}(s, a), \text{clip}(r(\theta), 1 - \epsilon, 1 + \epsilon) A^{\pi_{\theta_\text{old}}}(s, a) \right) \right]
\end{equation}
The function $\text{clip}(r(\theta), 1 - \epsilon, 1 + \epsilon)$ clips the probability ratio to the range $[1 - \epsilon, 1 + \epsilon]$.
Therefore, the objective function takes the minimum one between the original value and the clipped value.
This ensures that the policy update does not deviate too far from the old policy, thus stabilizing the training process.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Off-Policy Gradient Methods} \label{chap2:sec3}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Off-policy methods allow the agent to learn from data collected by a different policy called the behavior policy.
As a result, the off-policy approach improves sample efficiency by enabling the reuse of past experiences and enhances exploration by allowing the agent to learn from data collected by a behavior policy.
In order to apply off-policy learning within the policy gradient framework, we can incorporate importance sampling into the policy gradient \cref{chap2:thm:pg}.
\begin{equation}
  \begin{aligned}
    \nabla_\theta J(\theta)
    &= \mathbb{E}_{\tau \sim \pi_\theta} [\nabla_\theta \log \pi_\theta(s, a) Q^{\pi_\theta}(s, a)] \\
    &= \mathbb{E}_{\tau \sim \beta} \left[ \frac{\pi_\theta(s, a)}{\beta(s, a)} \nabla_\theta \log \pi_\theta(s, a) Q^{\pi_\theta}(s, a) \right]
  \end{aligned}
\end{equation}
This allows to estimate the gradient using samples generated by a behavior policy $\beta$ instead of the current policy $\pi_\theta$.

\subsection{Soft Actor-Critic (SAC)}

Soft Actor-Critic (SAC) \cite{SAC1, SAC2, SAC3} is an off-policy actor-critic model.
In SAC, the policy is trained with the objective of maximizing the expected return and the entropy of the policy.
\begin{equation}
  J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum^T_{t = 0} (r_t + \alpha \mathcal{H}(\pi_\theta(\cdot|s_t))) \right]
\end{equation}
\begin{equation}
  \mathcal{H}(\pi_\theta(\cdot|s_t)) = \mathbb{E}_{a_t \sim \pi_\theta}[-\log \pi_\theta(a_t|s_t)]
\end{equation}
where $\mathcal{H}(\cdot)$ is the entropy measure and $\alpha$ controls how important the entropy term is relative to the reward.
Entropy maximization leads to policies that can explore more and capture multiple modes of near-optimal strategies.
If there exist multiple options that seem equally good, the policy should assign each of them an equal probability.